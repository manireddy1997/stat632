covbetahat <- sigma2hat * solve(t(X) %*% X)
sebetahat <- sqrt(diag(covbetahat))
#standard error
sebetahat
coef(lm1)
#standard error
sebetahat
coef(lm1)
#standard error
sebetahat
coef(lm1)
summary(lm1)
summary(lm1)$coef
summary(lm1)$coef[,2]
#standard error
sebetahat
summary(lm1)$coef[,2]
#covariance
covbetahat
vcov(lm1)
#covariance
covbetahat
vcov(lm1)
knitr::opts_chunk$set(echo = TRUE)
df <- read.csv("data/hdi2018.csv")
lm1 <- lm(hdi_2018 ~ median_age + pctpop65 + pct_internet + pct_labour,data=df)
summary(lm1)
summary(lm1)
lm2<-lm(hdi_2018~1,data=df)
anova(lm2,lm1)
anova(lm2,lm1)
lm3<-lm(hdi_2018~ median_age+ pct_internet,data=df)
anova(lm3,lm1)
knitr::kable(c(1,2),col.names = c('lm1','lm2'))
knitr::kable(c([1,2]),col.names = c('lm1','lm2'))
knitr::kable(c([1,2]),col.names = c('lm1','lm2'))
knitr::kable(c(1,2),col.names = c('lm1','lm2'))
library(mtcars)
library("mtcars")
?mtcars
head(mtcars[, 1:4])
knitr::kable(data.frame(1,2),col.names = c('lm1','lm2'))
knitr::kable(data.frame(1,2),col.names = c('lm1','lm2'),simple)
knitr::kable(data.frame(1,2),col.names = c('lm1','lm2'),"simple")
knitr::kable(data.frame(summary(lm1)$adj.r.squared,summary(lm3)$adj.r.squared),col.names = c('lm1','lm2'),"simple")
knitr::kable(data.frame(summary(lm1)$adj.r.squared,summary(lm3)$adj.r.squared),col.names = c('lm1','lm2'),"pipe")
knitr::kable(data.frame(summary(lm1)$adj.r.squared,summary(lm3)$adj.r.squared),col.names = c('lm1','lm2'),"rst")
knitr::kable(data.frame(summary(lm1)$adj.r.squared,summary(lm3)$adj.r.squared),col.names = c('lm1','lm2'),"simple")
pairs(hdi_2018~median_age+ pct_internet,data=df)
plot(predict(lm3),resid(lm3),main = "Residuals vs Fitted",xlab="fitted values",ylab="residuals")
abline(h=0)
qqnorm(resid(lm3))
qqline(resid(lm3))
par(mar=c(4,4,1,1),mfrow=c(1,2))
plot(predict(lm3),resid(lm3),main = "Residuals vs Fitted",xlab="fitted values",ylab="residuals")
abline(h=0)
# qqplot
qqnorm(resid(lm3))
qqline(resid(lm3))
plot(rstandard(lm3),hatvalues(lm3))
abline(h=6/nrow(df))
df[which(hatvalues(lm3)>6/nrow(df)),]
knitr::opts_chunk$set(echo = TRUE)
project_df <- read.csv("data/California_Houses.csv")
View(project_df)
library(pacman)
pacman::p_load(funModeling)
metadata <- funModeling::df_status(project_df, print_results = FALSE)
knitr::kable(meta_loans)
knitr::kable(metadata)
View(metadata)
View(metadata)
knitr::opts_chunk$set(echo = TRUE)
project_df <- read.csv("data/California_Houses.csv")
replicate(1,10)
replicate(4,20)
knitr::opts_chunk$set(echo = TRUE)
germi_df <- read.table("data/pr8.1")
View(germi_df)
germi_df <- read.table("data/pr8.1",headers=TRUE)
?read.table
germi_df <- read.table("data/pr8.1",header=TRUE)
View(germi_df)
delam_df <- read.table("data/pr8.4",header=TRUE)
View(delam_df)
germi_df <- within(germi_df,{f_week = as.factor(week);f_water = as.factor(water)})
head(germi_df)
str(germi_df)
#a = 2
#b = 5
#g = 2*5
# N = 30
# Check balance
tapply(germi_df$y,germi_df$f_week:germi_df$f_water,length)
# fit the model
fit1 <- aov(y ~ f_week * f_water,data = germi_df)
# checking assumptions
par(mfrow = c(1,2))
plot(fit1,1:2)
par(mfrow = c(1,2))
plot(fit1,1:2)
library(pacman)
pacman::p_load(car)
# Levene Test
leveneTest(y ~ f_week * f_water,data = germi_df)
# Normality test
shapiro.test(fit1$residuals)
anova(fit1)
fit2 <- aov(y ~ f_week + f_water, data = germi_df)
# checking assumptions
par(mfrow = c(1,2))
plot(fit2,1:2)
# Levene Test
leveneTest(y ~ f_week + f_water,data = germi_df)
# Normality test
shapiro.test(fit2$residuals)
with(germi_df,interaction.plot(x.factor = f_week,trace.factor = f_water, response = y, type="b",col = "black","red"),pch = c(19,17))
with(germi_df,interaction.plot(x.factor = f_week,trace.factor = f_water, response = y, type="b",col = c("black","red"),pch = c(19,17)))
boxplot(y ~ f_week + f_water, data = germi_df)
pacman::p_load(car,emmeans)
em <- emmeans(fit2, pairwise ~ f_week | f_water)
summary(em$contrasts)
plot(summary(em$contrasts))
summary(em$contrasts)
plot(em$contrasts)
summary(em$contrasts)
em <- emmeans(fit2, pairwise ~ f_water | f_week)
summary(em$contrasts)
plot(em$contrasts)
em <- emmeans(fit2, pairwise ~ f_week | f_water)
summary(em$contrasts)
plot(em$contrasts)
delam_df <- read.table("data/pr8.4",header=TRUE)
delam_df <- within(delam_df,{f_flow = as.factor(flow);f_temp = as.factor(temp);f_laser = as.factor(laser)})
head(delam_df)
str(delam_df)
#a=2
#b=2
#c=2
#g = a*b*c = 2*2*2 = 8
#N =  16
#check data balance
tapply(delam_df$y, delam_df$f_flow:delam_df$f_temp:delam_df$f_laser, length)
#fit the model
fit3 <- aov(y ~ f_flow*f_temp * f_laser,data = delam_df)
par(mfrow = c(1, 3))
plot(fit3, 1:2)
hist(resid(fit3))
# Levene test
leveneTest(y ~ f_flow*f_temp * f_laser,data = delam_df)
shapiro.test(fit3$residuals)
boxCox(fit3)
ptf <- powerTransform(y ~ f_flow*f_temp * f_laser,data = delam_df)
summary(ptf)
delam_df$y_new <- bcPower(y,ptf$roundlam)
delam_df$y_new <- bcPower(delam_df$y,ptf$roundlam)
mod_fit <- aov(y_new ~ f_flow*f_temp * f_laser,data = delam_df)
par(mfrow = c(1, 2))
plot(mod_fit, 1:2)
# Levene test for equal variance
leveneTest(y_new ~ f_flow*f_temp * f_laser,data = delam_df)
# Shapiro-wilk test for normality
shapiro.test(mod_fit$residuals)
# Levene test for equal variance
leveneTest(y_new ~ f_flow*f_temp * f_laser,data = delam_df)
par(mfrow = c(1, 2))
plot(mod_fit, 1:2)
anova(mod_fit)
# Shapiro-wilk test for normality
shapiro.test(mod_fit$residuals)
# Levene test for equal variance
leveneTest(y_new ~ f_flow*f_temp * f_laser,data = delam_df)
ptf$roundlam
# interaction plot
with(delam_df,interaction.plot(x.factor = f_flow,trace.factor=f_temp,groups.factor = f_laser,response = y_new,type = "b",col=c("black","red","blue")))
# interaction plot
with(delam_df,interaction.plot(x.factor = f_flow,trace.factor=f_temp,second.trace.factor = f_laser,response = y_new,type = "b",col=c("black","red","blue"),pch=c(19,17,15)))
?interaction.plot
boxplot(y_new ~ f_flow*f_temp * f_laser,data = delam_df)
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
pload(ggplot2)
pacman::pload(ggplot2)
pacman::p_load(ggplot2)
df <- <- read.csv("county_votes16.csv")
df <- read.csv("county_votes16.csv")
df <- read.csv("county_votes16.csv")
df <- read.csv("data/county_votes16.csv")
glm1 <- glm(trump_win ~ obama_pctvotes, data = county_votes16, family = binomial)
glm1 <- glm(trump_win ~ obama_pctvotes, data = df, family = binomial)
summary(glm_1)
summary(glm1)
seq(min(df$obama_pctvotes)
f
seq(min(df$obama_pctvotes), max(df$obama_pctvotes))
?seq
min(df$obama_pctvotes)
new_predict <- data.frame(obama_pctvotes = seq(min(df$obama_pctvotes), max(df$obama_pctvotes)))
new_predict$pred_prob <- predict(glm1, newdata = new_predict, type = "response")
# Create scatter plot and add logistic curve
ggplot(data = df, aes(x = obama_pctvotes, y = trump_win)) +
geom_point() +
geom_line(data = new_predict, aes(x = obama_pctvotes, y = pred_prob)) +
labs(x = "Percent of Votes for Obama ", y = "Trump Win")
new <- data.frame(obama_pctvotes = c(40, 50, 60))
new$predicted_prob <- predict(glm_1, newdata = new_data, type = "response")
new$predicted_prob <- predict(glm1, newdata = new, type = "response")
print(new$predicted_prob)
new$predicted_prob
summary(glm1)
glm2<-glm(trump_win~pct_pop65+pct_black+pct_white+pct_hispanic+pct_asian+highschool+bachelors+income,data=df)
summary(glm2)
new_glm2 <- glm(trump_win~pct_black+pct_white+pct_hispanic+pct_asian+bachelors+income,data=df)
summary(new_glm2)
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
pacman::p_load(ggplot2,Stat2Data)
df <- Backpack
df <- data("Backpack")
df <- Backpack
pairs(BackpackWeight ~ .,data = df)
pacman::p_load(ggplot2,Stat2Data,MASS,car)
lm1 <- glm(BackpackWeight ~ .,data = df)
boxCox(lm1,lambda = seq(0.3,0.65,by=0.05))
boxCox(lm1)
summary(powerTransform(lm1))
boxCox(lm1)
boxCox(lm1,lambda=seq(0.1,0.1,by=0.05))
boxCox(lm1,lambda=seq(0.2,0.5,by=0.05))
boxCox(lm1,lambda=seq(0.2,0.9,by=0.05))
boxCox(lm1,lambda=seq(0.2,1,by=0.05))
boxCox(lm1,lambda=seq(0.3,1,by=0.05))
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
summary(powerTransform(lm1))
summary(lm1)
summary(powerTransform(lm1))
summary(glm((BackpackWeight^0.74) ~ .,data = df))
# Updating the model based on b
lm2 <- glm((BackpackWeight^0.74) ~ .,data = df)
# Updating the model based on b
lm_full <- glm((BackpackWeight^0.74) ~ .,data = df)
lm2 <- step(lm_full)
summary(lm2)
lm2 <- step(lm_full)
summary(lm2)
lm2 <- step(lm_full)
df <- Backpack
library(pacman)
pacman::p_load(ggplot2,Stat2Data,MASS,car)
data("Backpack")
df <- Backpack
pairs(BackpackWeight ~ .,data = df)
lm1 <- glm(BackpackWeight ~ .,data = df)
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
summary(powerTransform(lm1))
# Updating the model based on b
lm_full <- glm((BackpackWeight^0.74) ~ .,data = df)
lm2 <- step(lm_full)
lm2
summary(lm1)
lm1 <- lm(BackpackWeight ~ .,data = df)
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
summary(powerTransform(lm1))
# Updating the model based on b
lm_full <- lm((BackpackWeight^0.74) ~ .,data = df)
lm2 <- step(lm_full)
lm2
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
lm1 <- lm(BackpackWeight ~ .,data = df)
data("Backpack")
df <- Backpack
pairs(BackpackWeight ~ .,data = df)
lm1 <- lm(BackpackWeight ~ .,data = df)
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
summary(powerTransform(lm1))
lm1 <- glm(BackpackWeight ~ .,data = df)
boxCox(lm1,lambda=seq(0.4,1,by=0.05))
summary(powerTransform(lm1))
# Updating the model based on b
lm_full <- glm((BackpackWeight^0.74) ~ .,data = df)
lm2 <- step(lm_full)
lm2
lm_units <- lm(BackpackWeight^0.74) ~ .-Units,data = df)
lm_units <- lm(BackpackWeight^0.74) ~ .- Units,data = df)
lm_units <- lm(BackpackWeight^0.74) ~ . - Units,data = df)
lm_units <- lm(BackpackWeight^0.74) ~ BodyWeight+Ratio+BackProblems+manjor+Year+Sex+Status,data = df)
lm_units <- lm((BackpackWeight^0.74) ~ BodyWeight+Ratio+BackProblems+manjor+Year+Sex+Status,data = df)
lm_units <- lm((BackpackWeight^0.74) ~ BodyWeight+Ratio+BackProblems+major+Year+Sex+Status,data = df)
lm_units <- lm((BackpackWeight^0.74) ~ BodyWeight+Ratio+BackProblems+Major+Year+Sex+Status,data = df)
summary(lm_units)
anova(lm_units,lm_full)
lm_year <- lm((BackpackWeight^0.74) ~ . - Year,data = df)
anova(lm_year,lm_full)
lm_Year <- lm((BackpackWeight^0.74) ~ . - Major,data = df)
anova(lm_Year,lm_full)
lm_major <- lm((BackpackWeight^0.74) ~ . - Major,data = df)
anova(lm_major,lm_full)
lm_BP <- lm((BackpackWeight^0.74) ~ . - BackProblems,data = df)
anova(lm_BP,lm_full)
final_model <- lm((BackpackWeight^0.74) ~ BodyWeight+Ratio+Sex+Status,data = df)
final_model <- lm((BackpackWeight^0.74) ~ BodyWeight+Ratio+Sex+Status,data = df)
# Constant variance
par(mfrow = c(1,2),mar=c(4.5,4.5,2,2))
plot(predict(final_model),rstandard(final_model),xlab="Fitted values",ylab = "Standardized Residuals")
abline(h=0)
# Normality
qqnorm(rstandard(full_model))
# Constant variance
par(mfrow = c(1,2),mar=c(4.5,4.5,2,2))
plot(predict(final_model),rstandard(final_model),xlab="Fitted values",ylab = "Standardized Residuals")
abline(h=0)
# Normality
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
final_model <- lm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+Sex+Status,data = df)
# Constant variance
par(mfrow = c(1,2),mar=c(4.5,4.5,2,2))
plot(predict(final_model),rstandard(final_model),xlab="Fitted values",ylab = "Standardized Residuals")
abline(h=0)
# Normality
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
final_model <- lm((BackpackWeight^0.74) ~ sqrt(BodyWeight)+Ratio+Sex+Status,data = df)
# Constant variance
par(mfrow = c(1,2),mar=c(4.5,4.5,2,2))
plot(predict(final_model),rstandard(final_model),xlab="Fitted values",ylab = "Standardized Residuals")
abline(h=0)
# Normality
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
final_model <- lm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+Sex+Status,data = df)
final_model <- lm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+Sex+Status,data = df)
# Constant variance
par(mfrow = c(1,2),mar=c(4.5,4.5,2,2))
plot(predict(final_model),rstandard(final_model),xlab="Fitted values",ylab = "Standardized Residuals")
abline(h=0)
# Normality
qqnorm(rstandard(final_model))
qqline(rstandard(final_model))
# Updating the model based on b
lm_full <- glm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+BackProblems+Major+Year+Sex+Status+log(Units),data = df)
# Updating the model based on b
lm_full <- glm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+BackProblems+Major+Year+Sex+Status+Units,data = df)
lm2 <- step(lm_full)
lm2
View(df)
new_data <- data.frame(BodyWeight = 150,Ratio = 0.055,BacckProblems = 1, Major = "Bio",Year=3,Sex="Female", Status = "U",Units = 14)
predict(lm_full,newdata = new_data)
new_data <- data.frame(BodyWeight = 150,Ratio = 0.055,BackProblems = 1, Major = "Bio",Year=3,Sex="Female", Status = "U",Units = 14)
predict(lm_full,newdata = new_data)
round(predict(lm_full,newdata = new_data),0)
p2_glm <- glm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+BackProblems+Major+Year+Sex+Status+Units,data = df,family = binomial)
df$BackProblems<- as.factor(df$BackProblems)
p2_glm <- glm((BackpackWeight^0.74) ~ log(BodyWeight)+Ratio+BackProblems+Major+Year+Sex+Status+Units,data = df,family = binomial)
p2_glm <- glm(BackProblems ~ BackpackWeight+BodyWeight+Ratio+Major+Year+Sex+Status+Units,data = df,family = binomial)
df <- Backpack
p2_glm <- glm(BackProblems ~ BackpackWeight+BodyWeight+Ratio+Major+Year+Sex+Status+Units,data = df,family = binomial)
p2_glm <- glm(BackProblems ~ BackpackWeight+BodyWeight+Ratio+Major+Year+Sex+Status+Units,data = df,family = binomial)
reduced <- step(p2_glm)
reduced <- step(p2_glm)
reduced
new_x <- data.frame(BackpackWeight = 10,BodyWeight = 150,Ratio = 0.055, Major = "Bio",Year=3,Sex="Female", Status = "U",Units = 14)
predict(glm1,newdata=new_x,type="response")
predict(p2_glm,newdata=new_x,type="response")
round(predict(p2_glm,newdata=new_x,type="response"))
round(predict(p2_glm,newdata=new_x,type="response"),2)
predict(p2_glm,newdata=new_x,type="response")
pacman::p_load(ggplot2,Stat2Data,MASS,car,PROC)
library(PROC)
library(pROC)
pacman::p_load(ggplot2,Stat2Data,MASS,car,pROC)
?roc
roc_obj <- roc(df$BackProblems,new_x)
roc_obj <- roc(df$BackProblems,df)
roc_obj <- roc(df$BackProblems)
roc_obj <- roc(df,BackProblems,new_x)
roc_obj <- roc(df$BackProblems,predict(p2_glm,newdata=new_x,type="response"))
roc_obj <- roc(df$BackProblems,predict(p2_glm,newdata=new_x,type="response"))
install.packages("DiagrammeR")
library(DiagrammeR)
# Create nodes for each factor and level
nodes <- create_node_df(
label = c("A1", "A2", "A3", "B1", "B2", "B3", "C1", "C2", "D1", "D2"),
shape = rep("box", 10)
)
# Define edges for each factor
edges <- create_edge_df(
from = c(1, 2, 3, 4, 5, 6, 7, 7, 8, 8),
to = c(7, 7, 7, 8, 8, 8, 9, 10, 9, 10)
)
# Create the graph
graph <- create_graph(nodes_df = nodes, edges_df = edges)
# Create nodes for each factor and level
nodes <- create_node_df(
label = c("A1", "A2", "A3", "B1", "B2", "B3", "C1", "C2", "D1", "D2"),
shape = rep("box", 10)
)
?create_node_df
# Define edges for each factor
edges <- create_edge_df(n=72,
from = c(1, 2, 3, 4, 5, 6, 7, 7, 8, 8),
to = c(7, 7, 7, 8, 8, 8, 9, 10, 9, 10)
)
# Create the graph
graph <- create_graph(nodes_df = nodes, edges_df = edges)
# Plot the graph
render_graph(graph)
# Define edges for each factor
edges <- create_edge_df(n=72,
from = c(1, 2, 3, 4, 5, 6, 7, 7, 8, 8),
to = c(7, 7, 7, 8, 8, 8, 9, 10, 9, 10)
)
# Create the graph
graph <- create_graph(nodes_df = nodes, edges_df = edges)
# Create nodes for each factor and level
nodes <- create_node_df(
label = c("A1", "A2", "A3", "B1", "B2", "B3", "C1", "C2", "D1", "D2"),
shape = rep("box", 10)
)
# Create nodes for each factor and level
nodes <- create_node_df(n=72
label = c("A1", "A2", "A3", "B1", "B2", "B3", "C1", "C2", "D1", "D2"),
# Create nodes for each factor and level
nodes <- create_node_df(n=72,
label = c("A1", "A2", "A3", "B1", "B2", "B3", "C1", "C2", "D1", "D2"),
shape = rep("box", 10)
)
install.packages("Hasse")
library(Hasse)
library(ggm)
install.packages("ggm")
install.packages("graph")
library(ggm)
library(ggplot2)
# Define the factors and levels
A <- factor(rep(1:3, 24), levels = 1:3, labels = paste0("A", 1:3))
B <- factor(rep(rep(1:3, each = 8), 2), levels = 1:3, labels = paste0("B", 1:3))
C <- factor(rep(rep(1:2, each = 12), each = 3), levels = 1:2, labels = paste0("C", 1:2))
D <- factor(rep(rep(1:2, each = 36), each = 1), levels = 1:2, labels = paste0("D", 1:2))
# Define the data frame
data <- data.frame(A = A, B = B, C = C, D = D)
# Compute the Hasse diagram
graph <- hasse(data, ~A + B + C + D)
# Define the data frame
data <- data.frame(A = A, B = B, C = C, D = D)
# Define the levels of each factor
A <- factor(rep(1:3, each=12))
B <- factor(rep(1:3, times=4, each=4))
C <- factor(rep(rep(1:2, each=6), times=3))
D <- factor(rep(rep(1:2, each=3), times=6))
# Create a data frame with the factor levels
data <- data.frame(A=A, B=B, C=C, D=D)
install.packages("ggpubr")
install.packages("ggpubr")
library(ggplot2)
library(ggpubr)
# create a data frame with all possible combinations of factor levels
df <- expand.grid(A = 1:3, B = 1:3, C = 1:2, D = 1:2)
# assign factor types
df$A <- factor(df$A, levels = 1:3, ordered = TRUE)
df$B <- factor(df$B, levels = 1:3, ordered = TRUE)
df$C <- factor(df$C, levels = 1:2)
df$D <- factor(df$D, levels = 1:2)
# plot Hasse diagram
ggHasse(df, id = c("A", "B", "C", "D"), group = "C:D", size = 10)
library(ggpubr)
# plot Hasse diagram
ggHasse(df, id = c("A", "B", "C", "D"), group = "C:D", size = 10)
library(ggm)
# Define the factors and levels
A <- factor(rep(1:3, each = 12))
B <- factor(rep(1:3, times = 12))
C <- factor(rep(1:2, each = 6, times = 3))
D <- factor(rep(1:2, each = 36))
# Create the Hasse diagram
graph <- Hasse(A, B, C, D)
library(ggm)
# Create the Hasse diagram
graph <- Hasse(A, B, C, D)
# Plot the diagram
plot(graph, layout = layout.circle, vertex.label.cex = 1.2)
install.packages("igraph")
install.packages("igraph")
install.packages("igraph")
Hasse <- function(nA, nB, nC, nD){
# Define the vertices
vertices <- data.frame(A = rep(1:nA, nB*nC*nD),
B = rep(rep(1:nB, each=nA), nC*nD),
C = rep(rep(rep(1:nC, each=nA*nB), nD), nA),
D = rep(rep(rep(1:nD, each=nA*nB*nC), nB), nA))
# Define the edges
edges <- data.frame(from = 1:(nA*nB*nC*nD), to = 1:(nA*nB*nC*nD))
for (i in 1:(nA*nB*nC*nD)) {
a <- vertices[i, "A"]
b <- vertices[i, "B"]
c <- vertices[i, "C"]
d <- vertices[i, "D"]
edges[i, "to"] <- which(vertices$A == a & vertices$B == b & vertices$C == c & vertices$D == (d %% nD) + 1)
}
# Create the graph
graph <- graph_from_data_frame(edges, directed = TRUE, vertices = vertices)
# Plot the graph
plot(graph, layout=layout_as_tree)
}
# Create the Hasse diagram
graph <- Hasse(A, B, C, D)
knitr::opts_chunk$set(echo = TRUE)
library(pacman)
pacman::p_load(ggplot2)
data_df <- read.csv("./data/California_Houses.csv")
pairs(Median_House_Value ~ .,data = data_df)
